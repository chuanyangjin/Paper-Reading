# Paper Reading
This repository contains my reading notes for some impressive papers in recent years. Some are listed below by topic.

### Computer Vision
| Paper | Year | Citation |
| -- | -- | --: |
| DDPM: **Denoising Diffusion Probabilistic Models** | 2020 | ![citation](https://img.shields.io/badge/dynamic/json?label=citation&query=citationCount&url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2F289db3be7bf77e06e75541ba93269de3d604ac72%3Ffields%3DcitationCount) |
| MoCo: **Momentum Contrast for Unsupervised Visual Representation Learning** | 2020 | ![citation](https://img.shields.io/badge/dynamic/json?label=citation&query=citationCount&url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2Fadd2f205338d70e10ce5e686df4a690e2851bdfc%3Ffields%3DcitationCount) |
| ViT: **An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale** | 2021 | ![citation](https://img.shields.io/badge/dynamic/json?label=citation&query=citationCount&url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2F268d347e8a55b5eb82fb5e7d2f800e33c75ab18a%3Ffields%3DcitationCount) |
| **Swin Transformer: Hierarchical Vision Transformer using Shifted Windows** | 2021 | ![citation](https://img.shields.io/badge/dynamic/json?label=citation&query=citationCount&url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2Fc8b25fab5608c3e033d34b4483ec47e68ba109b7%3Ffields%3DcitationCount) |
| MAE: **Masked Autoencoders Are Scalable Vision Learners** | 2022 | ![citation](https://img.shields.io/badge/dynamic/json?label=citation&query=citationCount&url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2F6351ebb4a3287f5f3e1273464b3b91e5df5a16d7%3Ffields%3DcitationCount) |
| DALLÂ·E 2: **Hierarchical Text-Conditional Image Generation with CLIP Latents** | 2022 | ![citation](https://img.shields.io/badge/dynamic/json?label=citation&query=citationCount&url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2Fc57293882b2561e1ba03017902df9fc2f289dea2%3Ffields%3DcitationCount) |


### Natural Language Processing

| Paper | Year | Citation |
| -- | -- | --: |
| Transformer: **Attention Is All You Need** | 2017 | ![citation](https://img.shields.io/badge/dynamic/json?label=citation&query=citationCount&url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2F204e3073870fae3d05bcbc2f6a8e263d9b72e776%3Ffields%3DcitationCount) |
| **BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding** | 2018 | ![citation](https://img.shields.io/badge/dynamic/json?label=citation&query=citationCount&url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2Fdf2b0e26d0599ce3e70df8a9da02e51594e0e992%3Ffields%3DcitationCount) |
| GPT: **Improving Language Understanding by Generative Pre-Training** | 2018 | ![citation](https://img.shields.io/badge/dynamic/json?label=citation&query=citationCount&url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2Fcd18800a0fe0b668a1cc19f2ec95b5003d0a5035%3Ffields%3DcitationCount) |
| GPT-2: **Language Models are Unsupervised Multitask Learners** | 2019 | ![citation](https://img.shields.io/badge/dynamic/json?label=citation&query=citationCount&url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2F9405cc0d6169988371b2755e573cc28650d14dfe%3Ffields%3DcitationCount) |
| GPT-3: **Language Models are Few-Shot Learners** | 2020 | ![citation](https://img.shields.io/badge/dynamic/json?label=citation&query=citationCount&url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2F6b85b63579a916f705a8e10a49bd8d849d91b1fc%3Ffields%3DcitationCount) |
| InstructGPT: **Training Language Models to Follow Instructions with Human Feedback** | 2022 | ![citation](https://img.shields.io/badge/dynamic/json?label=citation&query=citationCount&url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2Fd766bffc357127e0dc86dd69561d5aeb520d6f4c%3Ffields%3DcitationCount) |


### Multimodal Learning
| Paper | Year | Citation |
| -- | -- | --: |
| CLIP: **Learning Transferable Visual Models From Natural Language Supervision** | 2021 | [![citation](https://img.shields.io/badge/dynamic/json?label=citation&query=citationCount&url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2F6f870f7f02a8c59c3e23f407f3ef00dd1dcf8fc4%3Ffields%3DcitationCount)](https://www.semanticscholar.org/paper/Learning-Transferable-Visual-Models-From-Natural-Radford-Kim/6f870f7f02a8c59c3e23f407f3ef00dd1dcf8fc4) |
| **ViLT: Vision-and-Language Transformer Without Convolution or Region Supervision** | 2021 | [![citation](https://img.shields.io/badge/dynamic/json?label=citation&query=citationCount&url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2F0839722fb5369c0abaff8515bfc08299efc790a1%3Ffields%3DcitationCount)](https://www.semanticscholar.org/paper/ViLT%3A-Vision-and-Language-Transformer-Without-or-Kim-Son/0839722fb5369c0abaff8515bfc08299efc790a1) |

ALBEF (2021), VLMo (2022), BLIP (2022), CoCa (2022), BEiTv3 (2022)
